# app.py
# -*- coding: utf-8 -*-

import re
import io
import glob
import math
import hashlib
from urllib.parse import urlparse
import os
import requests

import streamlit as st
import pandas as pd
import numpy as np
import chardet
import tldextract
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¸íŒ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ìë£Œ ì†ŒìŠ¤ íƒìƒ‰ê¸° GPT (SourceFinder)", layout="wide")

st.title("ìë£Œ ì†ŒìŠ¤ íƒìƒ‰ê¸° GPT (SourceFinder)")
st.caption(
    "ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ì„ 1ì°¨ ê·¼ê±°ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. ì •ë ¬ ê¸°ì¤€: "
    "(1) ì˜ë¯¸ ìœ ì‚¬ë„ (2) í‚¤ì›Œë“œ/ë„ë©”ì¸ ì§ë§¤ì¹­ (3) êµ­ë‚´/ê³µê³µ ê°€ì¤‘ì¹˜"
)

# ê¸°ì¡´
# DATA_DIR = "data"

# êµì²´
from pathlib import Path
APP_DIR = Path(__file__).parent.resolve()
DATA_DIR = (APP_DIR / "data").resolve()  # ì ˆëŒ€ê²½ë¡œë¡œ ê³ ì •
DEFAULT_TOPK = 10


# GitHub ì›ê²© ê¸°ë³¸ ì„¤ì • (Streamlit Cloudì—ì„œ .gitì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ìƒìˆ˜/secret ê¸°ë°˜)
DEFAULT_REPO_OWNER = st.secrets.get("REPO_OWNER", "letsgo999")
DEFAULT_REPO_NAME = st.secrets.get("REPO_NAME", "sourcefinder-app")
DEFAULT_REPO_BRANCH = st.secrets.get("REPO_BRANCH", "main")
DEFAULT_GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN", os.environ.get("GITHUB_TOKEN", ""))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_encoding(path: str) -> str:
    with open(path, "rb") as f:
        raw = f.read()
    res = chardet.detect(raw)
    return res.get("encoding") or "utf-8"


def find_url_column(df: pd.DataFrame) -> str | None:
    candidates = ["url", "urlì£¼ì†Œ", "ì£¼ì†Œ", "link", "ë§í¬"]
    for c in df.columns:
        cl = str(c).strip().lower()
        if any(k in cl for k in candidates) or cl == "url":
            return c
    return None


def normalize_text(x) -> str:
    if pd.isna(x):
        return ""
    return str(x)


def concat_fields(row: pd.Series, text_cols: list[str]) -> str:
    return " ".join(normalize_text(row.get(c, "")) for c in text_cols)


def has_hangul(s: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", s or ""))


def extract_domain(url: str) -> str:
    if not url or not isinstance(url, str):
        return ""
    try:
        ext = tldextract.extract(url)
        if ext.registered_domain:
            return ext.registered_domain
    except Exception:
        pass
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""


def make_markdown_table(rows: list[dict]) -> str:
    header = "| ìˆœìœ„ | ì¹´í…Œê³ ë¦¬ | ì‚¬ì´íŠ¸ëª… | URL | ì—°ê´€ì„± | í•œì¤„ ê·¼ê±° |\n|---:|---|---|---|---:|---|\n"
    lines = []
    for r in rows:
        name = r["ì‚¬ì´íŠ¸ëª…"]
        url = r["URL"]
        score = f"{r['ì—°ê´€ì„±']:.2f}"
        if url and isinstance(url, str) and url.startswith(("http://", "https://")):
            name_md = f"[{name}]({url})"
            url_md = url
        else:
            name_md = name
            url_md = "(URL ë¯¸í™•ì¸)"
        line = f"| {r['ìˆœìœ„']} | {r['ì¹´í…Œê³ ë¦¬']} | {name_md} | {url_md} | {score} | {r['í•œì¤„ ê·¼ê±°']} |"
        lines.append(line)
    return header + "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¡œë”© & ë³‘í•© (ë¡œì»¬ data/ â†’ ë¹„ì—ˆìœ¼ë©´ GitHub ì›ê²© data/)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _standardize_df(df: pd.DataFrame) -> pd.DataFrame:
    colmap = {}
    url_col = find_url_column(df)
    if url_col:
        colmap[url_col] = "URL"
    for c in df.columns:
        cl = str(c).strip().lower()
        if cl in ["site", "sitename", "name", "ì‚¬ì´íŠ¸", "ì‚¬ì´íŠ¸ëª…"]:
            colmap[c] = "ì‚¬ì´íŠ¸ëª…"
        if cl in ["category", "ì¹´í…Œê³ ë¦¬", "ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜"]:
            colmap[c] = "ì¹´í…Œê³ ë¦¬"
        if cl in ["notes", "ë©”ëª¨", "ê°„ëµë©”ëª¨", "ì„¤ëª…", "ë¹„ê³ "]:
            colmap[c] = "ê°„ëµë©”ëª¨"
    df2 = df.rename(columns=colmap)
    for col in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
        if col not in df2.columns:
            df2[col] = ""
    return df2[["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]]


def _read_local_files(data_dir: str) -> list[pd.DataFrame]:
    paths = []
    paths += glob.glob(f"{data_dir}/**/*.csv", recursive=True)
    paths += glob.glob(f"{data_dir}/**/*.xlsx", recursive=True)
    dfs: list[pd.DataFrame] = []
    for p in paths:
        try:
            if p.lower().endswith(".csv"):
                enc = detect_encoding(p)
                df = pd.read_csv(p, encoding=enc)
            else:
                df = pd.read_excel(p)
            dfs.append(_standardize_df(df))
        except Exception:
            continue
    return dfs


def _list_github_data_files(owner: str, repo: str, branch: str, subdir: str = "data") -> list[dict]:
    base = f"https://api.github.com/repos/{owner}/{repo}/contents/{subdir}?ref={branch}"
    stack = [base]
    files: list[dict] = []
    headers = {"Authorization": f"token {DEFAULT_GITHUB_TOKEN}"} if DEFAULT_GITHUB_TOKEN else {}
    while stack:
        url = stack.pop()
        try:
            r = requests.get(url, timeout=10, headers=headers)
            if r.status_code != 200:
                continue
            items = r.json()
            if not isinstance(items, list):
                continue
            for it in items:
                if it.get("type") == "file":
                    name = it.get("name", "").lower()
                    if name.endswith(".csv") or name.endswith(".xlsx"):
                        files.append({"download_url": it.get("download_url"), "name": it.get("name")})
                elif it.get("type") == "dir":
                    stack.append(it.get("url"))
        except Exception:
            continue
    return files


def _read_remote_files(file_meta: list[dict]) -> list[pd.DataFrame]:
    dfs: list[pd.DataFrame] = []
    headers = {"Authorization": f"token {DEFAULT_GITHUB_TOKEN}"} if DEFAULT_GITHUB_TOKEN else {}
    for it in file_meta:
        url = it.get("download_url")
        if not url:
            continue
        try:
            r = requests.get(url, timeout=20, headers=headers)
            if r.status_code != 200:
                continue
            data = r.content
            if it.get("name", "").lower().endswith(".csv"):
                enc = chardet.detect(data).get("encoding") or "utf-8"
                df = pd.read_csv(io.BytesIO(data), encoding=enc)
            else:
                df = pd.read_excel(io.BytesIO(data))
            dfs.append(_standardize_df(df))
        except Exception:
            continue
    return dfs


@st.cache_data(show_spinner=False)
def load_all_datasets(data_dir: Path) -> pd.DataFrame:
    import os
    data_dir = Path(data_dir)
    if not data_dir.exists():
        return pd.DataFrame(columns=["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"])

    # íŒŒì¼ ê²€ìƒ‰ (csv/xlsx)
    paths = list(data_dir.rglob("*.csv")) + list(data_dir.rglob("*.xlsx"))

    # ğŸ‘‰ ì§„ë‹¨ ë¡œê·¸: ì‚¬ì´ë“œë°”ì—ì„œ í™•ì¸ ê°€ëŠ¥
    st.session_state["_found_files"] = [str(p.relative_to(APP_DIR)) for p in paths]

    dfs = []
    for p in paths:
        try:
            if p.suffix.lower() == ".csv":
                enc = detect_encoding(str(p))
                df = pd.read_csv(p, encoding=enc)
            else:
                df = pd.read_excel(p)
        except Exception:
            continue

        colmap = {}
        url_col = find_url_column(df)
        if url_col:
            colmap[url_col] = "URL"
        for c in df.columns:
            cl = str(c).strip().lower()
            if cl in ["site", "sitename", "name", "ì‚¬ì´íŠ¸", "ì‚¬ì´íŠ¸ëª…"]:
                colmap[c] = "ì‚¬ì´íŠ¸ëª…"
            if cl in ["category", "ì¹´í…Œê³ ë¦¬", "ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜"]:
                colmap[c] = "ì¹´í…Œê³ ë¦¬"
            if cl in ["notes", "ë©”ëª¨", "ê°„ëµë©”ëª¨", "ì„¤ëª…", "ë¹„ê³ "]:
                colmap[c] = "ê°„ëµë©”ëª¨"

        df2 = df.rename(columns=colmap)
        for col in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
            if col not in df2.columns:
                df2[col] = ""
        dfs.append(df2[["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]])

    if not dfs:
        return pd.DataFrame(columns=["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"])

    merged = pd.concat(dfs, ignore_index=True).drop_duplicates()
    for col in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
        merged[col] = merged[col].fillna("").astype(str).str.strip()
    return merged



# âœ… ì„¸ì…˜ ì´ˆê¸°í™”: ë ˆí¬ì˜ data/ë§Œ ë¶ˆëŸ¬ì™€ ê³ ì •(ëˆ„êµ¬ë‚˜ ìƒˆë¡œê³ ì¹¨í•´ë„ ë™ì¼ ì‹œì‘ì )
if "base_df" not in st.session_state:
    st.session_state["base_df"] = load_all_datasets(DATA_DIR)
    if st.session_state["base_df"].empty:
        st.info("ê¸°ë³¸ data/ í´ë”ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•´ GitHub ì›ê²©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì›ê²©ì—ë„ ì—†ìœ¼ë©´ ì—…ë¡œë“œí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.")

# (ì„ íƒ) ì—…ë¡œë“œ í•´ì‹œ ì´ˆê¸°í™”
if "last_upload_hash" not in st.session_state:
    st.session_state["last_upload_hash"] = None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë­í‚¹ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_corpus(df: pd.DataFrame) -> tuple[list[str], list[str]]:
    text_cols = ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "ê°„ëµë©”ëª¨"]
    docs = [concat_fields(r, text_cols) for _, r in df.iterrows()]
    return docs, text_cols


def direct_match_score(row: pd.Series, tokens: list[str]) -> float:
    text = (
        normalize_text(row.get("ì¹´í…Œê³ ë¦¬", "")) + " "
        + normalize_text(row.get("ì‚¬ì´íŠ¸ëª…", "")) + " "
        + normalize_text(row.get("ê°„ëµë©”ëª¨", "")) + " "
        + normalize_text(row.get("URL", ""))
    ).lower()

    uniq = set(t for t in tokens if t)
    hit = sum(1 for t in uniq if t in text)
    dom = extract_domain(row.get("URL", ""))
    dom_hit = sum(1 for t in uniq if t in dom)

    raw = hit + 1.5 * dom_hit
    return 1 - math.exp(-0.4 * raw)  # 0~1 ìŠ¤ì¼€ì¼


def public_locale_boost(row: pd.Series, wants_public: bool, query_text: str) -> float:
    boost = 0.0
    url = normalize_text(row.get("URL", ""))
    dom = extract_domain(url)
    is_kr = dom.endswith(".kr") or ".kr/" in url.lower()
    is_go_kr = dom.endswith(".go.kr")
    name = normalize_text(row.get("ì‚¬ì´íŠ¸ëª…", ""))
    q = query_text.lower()
    hint_domestic = any(k in q for k in ["êµ­ë‚´", "í•œêµ­", "ì½”ë¦¬ì•„", "korea"])
    hint_public = any(k in q for k in ["ê³µê³µ", "ì •ë¶€", "stat", "í†µê³„", "data"])

    if wants_public and is_go_kr:
        boost += 0.6
    elif wants_public and is_kr:
        boost += 0.35
    if hint_domestic and (is_kr or has_hangul(name)):
        boost += 0.2
    if hint_public and (is_go_kr or "data.go.kr" in url.lower()):
        boost += 0.2

    return min(boost, 1.0)


def brief_reason(row: pd.Series, tokens: list[str], sim: float, dm: float, pb: float) -> str:
    reasons = []
    cat = normalize_text(row.get("ì¹´í…Œê³ ë¦¬", ""))
    if cat:
        reasons.append(f"ì¹´í…Œê³ ë¦¬ '{cat}'")
    hits = [
        t for t in tokens if t and (
            t in normalize_text(row.get("ì‚¬ì´íŠ¸ëª…", "")).lower()
            or t in normalize_text(row.get("ê°„ëµë©”ëª¨", "")).lower()
            or t in normalize_text(row.get("URL", "")).lower()
        )
    ]
    if hits:
        reasons.append(f"í‚¤ì›Œë“œ ë§¤ì¹­({', '.join(sorted(set(hits))[:3])})")
    dom = extract_domain(normalize_text(row.get("URL", "")))
    if dom.endswith(".go.kr"):
        reasons.append("êµ­ë‚´ ê³µê³µ ë„ë©”ì¸")
    if not reasons:
        reasons.append("í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ìƒìœ„")
    return " Â· ".join(reasons)[:80]


def rank_results(
    df: pd.DataFrame,
    query_text: str,
    wants_public: bool = True,
    selected_cats: list[str] | None = None,
    topk: int = 10,
) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"])

    if selected_cats:
        df = df[df["ì¹´í…Œê³ ë¦¬"].isin(selected_cats)].copy()
        if df.empty:
            return pd.DataFrame(columns=["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"])

    # 1) ì˜ë¯¸ ìœ ì‚¬ë„
    docs, _ = build_corpus(df)
    corpus = docs + [query_text]
    tf = TfidfVectorizer(min_df=1, ngram_range=(1, 2)).fit_transform(corpus)
    sims = cosine_similarity(tf[:-1], tf[-1]).reshape(-1)
    sims = np.clip(sims, 0, 1)

    # 2) ì§ë§¤ì¹­ / 3) ê³µê³µì§€í‘œ
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", query_text.lower())
    direct_scores, public_boosts = [], []
    for _, row in df.iterrows():
        direct_scores.append(direct_match_score(row, tokens))
        public_boosts.append(public_locale_boost(row, wants_public, query_text))
    direct_scores = np.array(direct_scores)
    public_boosts = np.array(public_boosts)

    # ìµœì¢… ì ìˆ˜
    final = 0.6 * sims + 0.3 * direct_scores + 0.1 * np.minimum(1.0, public_boosts)
    final = np.clip(final, 0, 1)

    out = df.copy().reset_index(drop=True)
    out["ì—°ê´€ì„±"] = final
    out["_sim"] = sims
    out["_dm"] = direct_scores
    out["_pb"] = public_boosts
    out["_domain"] = out["URL"].apply(extract_domain).fillna("")

    # ì ìˆ˜ ì •ë ¬ â†’ ë„ë©”ì¸ ì¤‘ë³µ ì œê±° â†’ ìƒìœ„ N
    out = out.sort_values("ì—°ê´€ì„±", ascending=False)
    dom = out["_domain"].fillna("").astype(str)
    site = out["ì‚¬ì´íŠ¸ëª…"].fillna("").astype(str)
    group_key = np.where(dom.str.len() == 0, site, dom)
    out = out.groupby(group_key).head(1)
    out = out.sort_values("ì—°ê´€ì„±", ascending=False).head(topk).copy()

    # í•œì¤„ ê·¼ê±°
    reasons = []
    for _, row in out.iterrows():
        reasons.append(brief_reason(row, tokens, row["_sim"], row["_dm"], row["_pb"]))
    out["í•œì¤„ ê·¼ê±°"] = reasons

    # í‘œì •ë¦¬
    out.insert(0, "ìˆœìœ„", range(1, len(out) + 1))
    out = out.drop(columns=["_sim", "_dm", "_pb", "_domain"], errors="ignore")
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´ë“œë°” (ê²€ìƒ‰ ì˜µì…˜ + ì—…ë¡œë“œ ìœ„ì ¯)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("ê²€ìƒ‰ ì˜µì…˜")
    query = st.text_input("ì§ˆì˜(í‚¤ì›Œë“œ)", value="êµ­ë‚´ ê³µê³µ ë°ì´í„° ì†Œë¹„ íŠ¸ë Œë“œ")
    boost_public = st.toggle("êµ­ë‚´ ê³µê³µ ë„ë©”ì¸(.go.kr, .kr) ê°€ì¤‘ì¹˜", value=True)
    topk = st.slider("Top N", min_value=5, max_value=50, value=DEFAULT_TOPK, step=1)

    st.markdown("---")
    st.caption("ë°ì´í„° ì¶”ê°€ (ì—…ë¡œë“œëŠ” í˜„ì¬ ì„¸ì…˜ì—ë§Œ ì ìš©ë©ë‹ˆë‹¤)")
    uploaded = st.file_uploader(
        "CSV/XLSX ì¶”ê°€ ì—…ë¡œë“œ(ì„ íƒ)",
        type=["csv", "xlsx"],
        key="uploader",
    )
    st.caption(f"ê¸°ë³¸ ë°ì´í„°ëŠ” ê¹ƒí—ˆë¸Œ {DEFAULT_REPO_OWNER}/{DEFAULT_REPO_NAME}@{DEFAULT_REPO_BRANCH} ì˜ data/ í´ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.markdown("---")
    if st.button("ë°ì´í„° ë‹¤ì‹œ ì½ê¸°(ìºì‹œ ì´ˆê¸°í™”)"):
        st.cache_data.clear()
        st.session_state["base_df"] = load_all_datasets(DATA_DIR)
        st.success("ìºì‹œ ì´ˆê¸°í™” ë° ì¬ë¡œë”© ì™„ë£Œ!")

    # ì§„ë‹¨ìš©: ì•±ì´ ì‹¤ì œë¡œ ë³¸ íŒŒì¼ ëª©ë¡
    if "_found_files" in st.session_state:
        with st.expander("ë°ì´í„° ì§„ë‹¨(ë°œê²¬ëœ íŒŒì¼)"):
            if st.session_state["_found_files"]:
                for p in st.session_state["_found_files"]:
                    st.write("â€¢", p)
            else:
                st.write("ë°œê²¬ëœ íŒŒì¼ ì—†ìŒ â€” ë ˆí¬ì˜ `data/` ê²½ë¡œì™€ íŒŒì¼ ì»¤ë°‹ì„ í™•ì¸í•˜ì„¸ìš”.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—…ë¡œë“œ ì²˜ë¦¬(ì„¸ì…˜ í•œì • ë³‘í•©, ë¬´í•œë£¨í”„ ë°©ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if uploaded is not None:
    try:
        up_bytes = uploaded.getvalue()
        fhash = hashlib.md5(up_bytes).hexdigest()
        if st.session_state.get("last_upload_hash") != fhash:
            if uploaded.name.lower().endswith(".csv"):
                enc = chardet.detect(up_bytes).get("encoding") or "utf-8"
                up_df = pd.read_csv(io.BytesIO(up_bytes), encoding=enc)
            else:
                up_df = pd.read_excel(io.BytesIO(up_bytes))

            url_col = find_url_column(up_df)
            colmap = {}
            if url_col:
                colmap[url_col] = "URL"
            for c in up_df.columns:
                cl = c.strip().lower()
                if cl in ["site", "sitename", "name", "ì‚¬ì´íŠ¸", "ì‚¬ì´íŠ¸ëª…"]:
                    colmap[c] = "ì‚¬ì´íŠ¸ëª…"
                if cl in ["category", "ì¹´í…Œê³ ë¦¬", "ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜"]:
                    colmap[c] = "ì¹´í…Œê³ ë¦¬"
                if cl in ["notes", "ë©”ëª¨", "ê°„ëµë©”ëª¨", "ì„¤ëª…", "ë¹„ê³ "]:
                    colmap[c] = "ê°„ëµë©”ëª¨"

            up_df2 = up_df.rename(columns=colmap)
            for col in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
                if col not in up_df2.columns:
                    up_df2[col] = ""
            up_df2 = up_df2[["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]]

            st.session_state["base_df"] = (
                pd.concat([st.session_state["base_df"], up_df2], ignore_index=True)
                .drop_duplicates()
            )
            st.session_state["last_upload_hash"] = fhash
            st.success(f"ì¶”ê°€ ë°ì´í„° ë³‘í•© ì™„ë£Œ! (í˜„ì¬ ì„¸ì…˜ ê¸°ì¤€ ì´ {len(st.session_state['base_df'])}ê±´)")
    except Exception as e:
        st.error(f"ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹´í…Œê³ ë¦¬ ì˜µì…˜ (í•­ìƒ ìµœì‹  ì„¸ì…˜ DF ê¸°ì¤€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    _cats = (
        st.session_state["base_df"]["ì¹´í…Œê³ ë¦¬"].dropna().astype(str).str.strip()
        if not st.session_state["base_df"].empty else pd.Series([], dtype=str)
    )
    sel_categories = sorted([c for c in _cats.unique() if c])
    selected_cats = st.multiselect("ì¹´í…Œê³ ë¦¬(ì„ íƒ)", sel_categories, default=[], key="cats")

    st.markdown("---")
    st.markdown("**ì—°ê´€ì„± ì ìˆ˜ ì½ê¸°**")
    st.markdown(
        "0.0 ~ 0.1 â†’ ê±°ì˜ ë¬´ê´€  \n"
        "0.1 ~ 0.2 â†’ ì•½ê°„ ê´€ë ¨  \n"
        "0.2 ~ 0.4 â†’ ë³´í†µ ê´€ë ¨  \n"
        "0.4 ~     â†’ ê°•í•˜ê²Œ ê´€ë ¨"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¸ë¬¸: ê²€ìƒ‰ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base_df = st.session_state["base_df"]

if base_df.empty:
    st.warning("ë ˆí¬ì™€ ì›ê²©ì˜ `data/`ì—ì„œ CSV/XLSXë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì„ ì»¤ë°‹í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
else:
    if st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary") or True:
        result_df = rank_results(
            base_df.copy(),
            query_text=query,
            wants_public=boost_public,
            selected_cats=selected_cats,
            topk=topk,
        )

        if result_df.empty:
            st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ì¹´í…Œê³ ë¦¬ í•„í„°ë¥¼ í•´ì œí•˜ì„¸ìš”.")
        else:
            rows = result_df[
                ["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"]
            ].to_dict(orient="records")
            md_table = make_markdown_table(rows)

            st.markdown("### ì—°ê´€ ìë£Œ ì†ŒìŠ¤")
            st.markdown(md_table, unsafe_allow_html=False)

            st.download_button(
                label="ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                data=result_df.to_csv(index=False).encode("utf-8-sig"),
                file_name="sourcefinder_results.csv",
                mime="text/csv",
            )
