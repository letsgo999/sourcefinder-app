# app.py
# -*- coding: utf-8 -*-
import re
import io
import glob
import math
from urllib.parse import urlparse

import streamlit as st
import pandas as pd
import numpy as np
import chardet
import tldextract

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

############################
# ê¸°ë³¸ ì„¤ì •
############################
st.set_page_config(page_title="ìë£Œ ì†ŒìŠ¤ íƒìƒ‰ê¸° GPT (SourceFinder)", layout="wide")

st.title("ìë£Œ ì†ŒìŠ¤ íƒìƒ‰ê¸° GPT (SourceFinder)")
st.caption("ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ì„ 1ì°¨ ê·¼ê±°ë¡œ, â€˜ì¹´í…Œê³ ë¦¬Â·ì‚¬ì´íŠ¸ëª…Â·URLÂ·ë©”ëª¨â€™ë¥¼ ê¸°ë°˜ ì •ë ¬í•©ë‹ˆë‹¤. "
           "ì •ë ¬ ê¸°ì¤€: (1) ì˜ë¯¸ ìœ ì‚¬ë„ (2) í‚¤ì›Œë“œ/ë„ë©”ì¸ ì§ë§¤ì¹­ (3) êµ­ë‚´/ê³µê³µ ê°€ì¤‘ì¹˜")

DATA_DIR = "data"
DEFAULT_TOPK = 10

############################
# ìœ í‹¸ í•¨ìˆ˜
############################
URL_CANDIDATE_COLS = ["url", "urlì£¼ì†Œ", "ì£¼ì†Œ", "link", "ë§í¬"]

def detect_encoding(path):
    with open(path, "rb") as f:
        raw = f.read()
    res = chardet.detect(raw)
    return res.get("encoding") or "utf-8"

def find_url_column(df: pd.DataFrame) -> str | None:
    cols = [c for c in df.columns]
    lc_map = {c: c.lower() for c in cols}
    for c in cols:
        cl = lc_map[c]
        if any(k in cl for k in URL_CANDIDATE_COLS):
            return c
    # ë°±ì—…: ì •í™•íˆ "URL" ê°™ì€ ì»¬ëŸ¼ ì¡´ì¬ ì‹œ
    for c in cols:
        if c.strip().lower() == "url":
            return c
    return None

def normalize_text(x):
    if pd.isna(x):
        return ""
    return str(x)

def concat_fields(row: pd.Series, text_cols: list[str]) -> str:
    return " ".join(normalize_text(row.get(c, "")) for c in text_cols)

def has_hangul(s: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", s or ""))

def extract_domain(url: str) -> str:
    if not url or not isinstance(url, str):
        return ""
    # tldextract: subdomain.domain.suffix â†’ registered_domain
    try:
        ext = tldextract.extract(url)
        if ext.registered_domain:
            return ext.registered_domain
    except Exception:
        pass
    # fallback
    try:
        netloc = urlparse(url).netloc
        return netloc.lower()
    except Exception:
        return ""

def make_markdown_table(rows: list[dict]) -> str:
    # rows: dict with keys ["ìˆœìœ„","ì¹´í…Œê³ ë¦¬","ì‚¬ì´íŠ¸ëª…","URL","ì—°ê´€ì„±","í•œì¤„ ê·¼ê±°"]
    # 'ì‚¬ì´íŠ¸ëª…'ì€ í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ë¡œ, URL ë¯¸í™•ì¸ ì‹œ plain í…ìŠ¤íŠ¸
    header = "| ìˆœìœ„ | ì¹´í…Œê³ ë¦¬ | ì‚¬ì´íŠ¸ëª… | URL | ì—°ê´€ì„± | í•œì¤„ ê·¼ê±° |\n|---:|---|---|---|---:|---|\n"
    lines = []
    for r in rows:
        name = r["ì‚¬ì´íŠ¸ëª…"]
        url = r["URL"]
        score = f"{r['ì—°ê´€ì„±']:.2f}"
        if url and url != "URL ë¯¸í™•ì¸" and isinstance(url, str) and url.startswith(("http://", "https://")):
            name_md = f"[{name}]({url})"
            url_md = url
        else:
            name_md = name
            url_md = "(URL ë¯¸í™•ì¸)"
        line = f"| {r['ìˆœìœ„']} | {r['ì¹´í…Œê³ ë¦¬']} | {name_md} | {url_md} | {score} | {r['í•œì¤„ ê·¼ê±°']} |"
        lines.append(line)
    return header + "\n".join(lines)

############################
# ë°ì´í„° ë¡œë”© & ë³‘í•© (ìºì‹œ)
############################
@st.cache_data(show_spinner=False)
def load_all_datasets(data_dir: str) -> pd.DataFrame:
    paths = []
    paths += glob.glob(f"{data_dir}/**/*.csv", recursive=True)
    paths += glob.glob(f"{data_dir}/**/*.xlsx", recursive=True)
    dfs = []

    for p in paths:
        try:
            if p.lower().endswith(".csv"):
                enc = detect_encoding(p)
                df = pd.read_csv(p, encoding=enc)
            else:
                df = pd.read_excel(p)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {p} ({e})")
            continue

        # í‘œì¤€ ì»¬ëŸ¼ ì •ë¦¬
        # ì‚¬ì´íŠ¸ëª…, ì¹´í…Œê³ ë¦¬, URL, ê°„ëµë©”ëª¨ê°€ ì´ìƒì ì¸ ìŠ¤í‚¤ë§ˆ
        colmap = {}
        # ì‚¬ì´íŠ¸ëª…
        for c in df.columns:
            if str(c).strip() in ["ì‚¬ì´íŠ¸ëª…", "site", "sitename", "name", "ì‚¬ì´íŠ¸"]:
                colmap[c] = "ì‚¬ì´íŠ¸ëª…"
        if "ì‚¬ì´íŠ¸ëª…" not in colmap.values():
            # ì¶”ì •: ì²« ë²ˆì§¸ ë¬¸ìì—´í˜• ì»¬ëŸ¼ì„ ì‚¬ì´íŠ¸ëª…ìœ¼ë¡œ ì‚¬ìš©
            for c in df.columns:
                if df[c].dtype == "object":
                    colmap[c] = "ì‚¬ì´íŠ¸ëª…"
                    break

        # ì¹´í…Œê³ ë¦¬
        for c in df.columns:
            if str(c).strip() in ["ì¹´í…Œê³ ë¦¬", "category", "ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜"]:
                colmap[c] = "ì¹´í…Œê³ ë¦¬"

        # URL
        url_col = find_url_column(df)
        if url_col:
            colmap[url_col] = "URL"

        # ê°„ëµë©”ëª¨
        for c in df.columns:
            if str(c).strip() in ["ê°„ëµë©”ëª¨", "ë©”ëª¨", "notes", "ì„¤ëª…", "ë¹„ê³ "]:
                colmap[c] = "ê°„ëµë©”ëª¨"

        df2 = df.copy()
        df2 = df2.rename(columns=colmap)

        # í‘œì¤€ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ìƒì„±
        for c in ["ì‚¬ì´íŠ¸ëª…", "ì¹´í…Œê³ ë¦¬", "URL", "ê°„ëµë©”ëª¨"]:
            if c not in df2.columns:
                df2[c] = ""

        dfs.append(df2[["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]])

    if not dfs:
        return pd.DataFrame(columns=["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"])

    merged = pd.concat(dfs, ignore_index=True)

    # ê³µë°±/NA ì •ë¦¬
    for c in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
        merged[c] = merged[c].fillna("").astype(str).str.strip()

    # ì™„ì „ ì¤‘ë³µ ì œê±°
    merged = merged.drop_duplicates()

    return merged

base_df = load_all_datasets(DATA_DIR)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì‚¬ì´ë“œë°” 1ì°¨ êµ¬ì„±: ì§ˆì˜/ê°€ì¤‘ì¹˜/TopN + íŒŒì¼ ì—…ë¡œë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("ê²€ìƒ‰ ì˜µì…˜")
    query = st.text_input("ì§ˆì˜(í‚¤ì›Œë“œ)", value="êµ­ë‚´ ê³µê³µ ë°ì´í„° ì†Œë¹„ íŠ¸ë Œë“œ")
    boost_public = st.toggle("êµ­ë‚´ ê³µê³µ ë„ë©”ì¸(.go.kr, .kr) ê°€ì¤‘ì¹˜", value=True)
    topk = st.slider("Top N", min_value=5, max_value=50, value=DEFAULT_TOPK, step=1)

    st.markdown("---")
    st.caption("ë°ì´í„° ì¶”ê°€")
    uploaded = st.file_uploader("CSV/XLSX ì¶”ê°€ ì—…ë¡œë“œ(ì„ íƒ)", type=["csv", "xlsx"])

# 2) ì—…ë¡œë“œ ë°ì´í„° ë³‘í•©(ìˆìœ¼ë©´)
if uploaded:
    try:
        if uploaded.name.lower().endswith(".csv"):
            up_bytes = uploaded.read()
            enc = chardet.detect(up_bytes).get("encoding") or "utf-8"
            up_df = pd.read_csv(io.BytesIO(up_bytes), encoding=enc)
        else:
            up_df = pd.read_excel(uploaded)

        url_col = find_url_column(up_df)
        colmap = {}
        if url_col:
            colmap[url_col] = "URL"
        for c in up_df.columns:
            cl = c.strip().lower()
            if cl in ["site", "sitename", "name", "ì‚¬ì´íŠ¸", "ì‚¬ì´íŠ¸ëª…"]:
                colmap[c] = "ì‚¬ì´íŠ¸ëª…"
            if cl in ["category", "ì¹´í…Œê³ ë¦¬", "ë¶„ë¥˜", "ëŒ€ë¶„ë¥˜"]:
                colmap[c] = "ì¹´í…Œê³ ë¦¬"
            if cl in ["notes", "ë©”ëª¨", "ê°„ëµë©”ëª¨", "ì„¤ëª…", "ë¹„ê³ "]:
                colmap[c] = "ê°„ëµë©”ëª¨"

        up_df2 = up_df.rename(columns=colmap)
        for c in ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]:
            if c not in up_df2.columns:
                up_df2[c] = ""
        up_df2 = up_df2[["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ê°„ëµë©”ëª¨"]]

        base_df = pd.concat([base_df, up_df2], ignore_index=True).drop_duplicates()
        st.success(f"ì¶”ê°€ ë°ì´í„° ë³‘í•© ì™„ë£Œ! (ì´ {len(base_df)}ê±´)")
    except Exception as e:
        st.error(f"ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# 3) ë³‘í•© ì´í›„ì— ì¹´í…Œê³ ë¦¬ ì˜µì…˜ ìƒì„± (ğŸ‘‰ ì—¬ê¸°ì„œ ìƒì„±!)
with st.sidebar:
    sel_categories = sorted([c for c in base_df["ì¹´í…Œê³ ë¦¬"].unique() if c])
    selected_cats = st.multiselect("ì¹´í…Œê³ ë¦¬(ì„ íƒ)", sel_categories, default=[])
with st.sidebar:
    sel_categories = sorted([c for c in base_df["ì¹´í…Œê³ ë¦¬"].unique() if c])
    selected_cats = st.multiselect("ì¹´í…Œê³ ë¦¬(ì„ íƒ)", sel_categories, default=[])

    # â† ì—¬ê¸°ì„œë„ ë°˜ë“œì‹œ ê°™ì€ ë ˆë²¨(ìŠ¤í˜ì´ìŠ¤ 4ì¹¸)
    st.markdown("---")
    st.markdown("**ì—°ê´€ì„± ì ìˆ˜ ì½ê¸°**")
    st.markdown(
        """
0.0 ~ 0.1 â†’ ê±°ì˜ ë¬´ê´€  
0.1 ~ 0.2 â†’ ì•½ê°„ ê´€ë ¨  
0.2 ~ 0.4 â†’ ë³´í†µ ê´€ë ¨  
0.4 ~     â†’ ê°•í•˜ê²Œ ê´€ë ¨
        """
    )

############################
# ë­í‚¹ ë¡œì§
############################
def build_corpus(df: pd.DataFrame) -> tuple[list[str], list[str]]:
    text_cols = ["ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "ê°„ëµë©”ëª¨"]
    docs = [concat_fields(r, text_cols) for _, r in df.iterrows()]
    return docs, text_cols

def direct_match_score(row: pd.Series, tokens: list[str]) -> float:
    # (2) í‚¤ì›Œë“œ/ë„ë©”ì¸ ì§ë§¤ì¹­ ì ìˆ˜
    text = (normalize_text(row.get("ì¹´í…Œê³ ë¦¬", "")) + " " +
            normalize_text(row.get("ì‚¬ì´íŠ¸ëª…", "")) + " " +
            normalize_text(row.get("ê°„ëµë©”ëª¨", "")) + " " +
            normalize_text(row.get("URL", ""))).lower()

    # í† í° ë§¤ì¹­ (ì¤‘ë³µ ì œê±°)
    uniq_tokens = set([t for t in tokens if t])
    hit = sum(1 for t in uniq_tokens if t in text)
    # ë„ë©”ì¸ ë‚´ í‚¤ì›Œë“œ ë§¤ì¹­ ê°€ì¤‘
    dom = extract_domain(row.get("URL", ""))
    dom_hit = sum(1 for t in uniq_tokens if t in dom)

    raw = hit + 1.5 * dom_hit
    # ì •ê·œí™” (ëŒ€ëµ 0~1 ë²”ìœ„)
    return 1 - math.exp(-0.4 * raw)  # ì™„ë§Œí•œ S-curve

def public_locale_boost(row: pd.Series, wants_public: bool, query_text: str) -> float:
    # (3) êµ­ë‚´/ê³µê³µ ê°€ì¤‘ì¹˜
    boost = 0.0
    url = normalize_text(row.get("URL", ""))
    dom = extract_domain(url)
    is_kr = dom.endswith(".kr") or ".kr/" in url.lower()
    is_go_kr = dom.endswith(".go.kr")
    name = normalize_text(row.get("ì‚¬ì´íŠ¸ëª…", ""))
    # ì§ˆì˜ì— êµ­ë‚´/í•œêµ­/ê³µê³µ í¬í•¨?
    q = query_text.lower()
    hint_domestic = any(k in q for k in ["êµ­ë‚´", "í•œêµ­", "ì½”ë¦¬ì•„", "korea"])
    hint_public = any(k in q for k in ["ê³µê³µ", "ì •ë¶€", "stat", "í†µê³„", "data"])

    if wants_public and is_go_kr:
        boost += 0.6
    elif wants_public and is_kr:
        boost += 0.35

    if hint_domestic and (is_kr or has_hangul(name)):
        boost += 0.2
    if hint_public and (is_go_kr or "data.go.kr" in url.lower()):
        boost += 0.2

    # ìƒí•œ
    return min(boost, 1.0)

def brief_reason(row: pd.Series, tokens: list[str], sim: float, dm: float, pb: float) -> str:
    reasons = []
    cat = normalize_text(row.get("ì¹´í…Œê³ ë¦¬", ""))
    if cat:
        reasons.append(f"ì¹´í…Œê³ ë¦¬ '{cat}'")
    hits = [t for t in tokens if t and (t in normalize_text(row.get("ì‚¬ì´íŠ¸ëª…","")).lower() 
                                        or t in normalize_text(row.get("ê°„ëµë©”ëª¨","")).lower()
                                        or t in normalize_text(row.get("URL","")).lower())]
    if hits:
        reasons.append(f"í‚¤ì›Œë“œ ë§¤ì¹­({', '.join(sorted(set(hits))[:3])})")
    url = normalize_text(row.get("URL", ""))
    dom = extract_domain(url)
    if dom.endswith(".go.kr"):
        reasons.append("êµ­ë‚´ ê³µê³µ ë„ë©”ì¸")
    if not reasons:
        reasons.append("í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ìƒìœ„")
    # ë„ˆë¬´ ê¸¸ë©´ ì»·
    txt = " Â· ".join(reasons)
    return txt[:80]

def rank_results(df: pd.DataFrame, query_text: str, wants_public=True, selected_cats=None, topk=10) -> pd.DataFrame:
    # ë¹ˆ ë°ì´í„° ê°€ë“œ
    if df is None or df.empty:
        return pd.DataFrame(columns=["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"])

    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if selected_cats:
        df = df[df["ì¹´í…Œê³ ë¦¬"].isin(selected_cats)].copy()
        if df.empty:
            return pd.DataFrame(columns=["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"])

    # 1) ì˜ë¯¸ ìœ ì‚¬ë„ (TF-IDF)
    docs, _ = build_corpus(df)
    corpus = docs + [query_text]
    vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
    tf = vectorizer.fit_transform(corpus)
    sims = cosine_similarity(tf[:-1], tf[-1]).reshape(-1)
    sims = np.clip(sims, 0, 1)

    # 2) í‚¤ì›Œë“œ/ë„ë©”ì¸ ì§ë§¤ì¹­
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", query_text.lower())
    direct_scores, public_boosts = [], []
    for _, row in df.iterrows():
        dm = direct_match_score(row, tokens)
        pb = public_locale_boost(row, wants_public, query_text)
        direct_scores.append(dm)
        public_boosts.append(pb)
    direct_scores = np.array(direct_scores)
    public_boosts = np.array(public_boosts)

    # 3) ìµœì¢… ì ìˆ˜
    final = 0.6 * sims + 0.3 * direct_scores + 0.1 * np.minimum(1.0, public_boosts)
    final = np.clip(final, 0, 1)

    out = df.copy().reset_index(drop=True)
    out["ì—°ê´€ì„±"] = final
    out["_sim"] = sims
    out["_dm"] = direct_scores
    out["_pb"] = public_boosts

    # ë„ë©”ì¸ ì¶”ì¶œ
    out["_domain"] = out["URL"].apply(extract_domain).fillna("")

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    out = out.sort_values("ì—°ê´€ì„±", ascending=False)

    # âœ… URL ì—†ìœ¼ë©´ ì‚¬ì´íŠ¸ëª…ìœ¼ë¡œ ê·¸ë£¹í™” (íƒ­/ìŠ¤í˜ì´ìŠ¤ í˜¼ìš© ê¸ˆì§€!)
    dom = out["_domain"].fillna("").astype(str)
    site = out["ì‚¬ì´íŠ¸ëª…"].fillna("").astype(str)
    group_key = np.where(dom.str.len() == 0, site, dom)
    out = out.groupby(group_key).head(1)

    # ìƒìœ„ N
    out = out.sort_values("ì—°ê´€ì„±", ascending=False).head(topk).copy()

    # í•œì¤„ ê·¼ê±°
    reasons = []
    for _, row in out.iterrows():
        reasons.append(brief_reason(row, tokens, row["_sim"], row["_dm"], row["_pb"]))
    out["í•œì¤„ ê·¼ê±°"] = reasons

    # ìˆœìœ„ ì»¬ëŸ¼ + ë‚´ë¶€ ì»¬ëŸ¼ ì •ë¦¬
    out.insert(0, "ìˆœìœ„", range(1, len(out) + 1))
    out = out.drop(columns=["_sim", "_dm", "_pb", "_domain"], errors="ignore")

    # ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ë³´ì¥
    cols = ["ìˆœìœ„", "ì¹´í…Œê³ ë¦¬", "ì‚¬ì´íŠ¸ëª…", "URL", "ì—°ê´€ì„±", "í•œì¤„ ê·¼ê±°"]
    return out[cols]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ê²°ê³¼ ë­í‚¹ & í‘œì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary") or query:
    if base_df.empty:
        st.error("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. data/ í´ë”ì— CSV/XLSX íŒŒì¼ì„ ë„£ê±°ë‚˜ ìƒë‹¨ì—ì„œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        result_df = rank_results(
            base_df.copy(),
            query_text=query,
            wants_public=boost_public,
            selected_cats=selected_cats,
            topk=topk
        )

        if result_df.empty:
            st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ì¹´í…Œê³ ë¦¬ í•„í„°ë¥¼ í•´ì œí•˜ì„¸ìš”.")
        else:
            rows = result_df[["ìˆœìœ„","ì¹´í…Œê³ ë¦¬","ì‚¬ì´íŠ¸ëª…","URL","ì—°ê´€ì„±","í•œì¤„ ê·¼ê±°"]].to_dict(orient="records")
            md_table = make_markdown_table(rows)
            st.markdown("### ì—°ê´€ ìë£Œ ì†ŒìŠ¤")
            st.markdown(md_table)

            csv_bytes = result_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button("ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", csv_bytes, "sourcefinder_results.csv", "text/csv")

        # ì¶”ê°€ íƒìƒ‰ íŒ (3ì¤„ ì´ë‚´)
        st.markdown("â€”")
        st.markdown("**ì¶”ê°€ íƒìƒ‰ íŒ**")
        st.markdown("- ì§ˆì˜ì— ì§€ì—­/ì‚°ì—…/ë°ì´í„°í˜•(ì˜ˆ: â€˜êµ­ë‚´ ê³µê³µ ì¹´ë“œë§¤ì¶œ í†µê³„ ì›”ê°„â€™)ì„ ë„£ìœ¼ë©´ ì •í™•ë„â†‘")
        st.markdown("- ê°™ì€ ë„ë©”ì¸ì€ 1ê°œë§Œ ë…¸ì¶œë©ë‹ˆë‹¤. ì„¸ë¶€ í˜ì´ì§€ëŠ” â€˜ê°„ëµë©”ëª¨â€™ í‚¤ì›Œë“œë¡œ ì¢í˜€ë³´ì„¸ìš”.")
        st.markdown("- ê³µê³µ ë„ë©”ì¸(.go.kr, .kr) ìš°ì„  ì˜µì…˜ì„ í† ê¸€í•˜ì—¬ ê³µê³µì„± ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì„¸ìš”.")
